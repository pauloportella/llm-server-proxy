# LLM Queue Proxy Configuration

# Global settings
queue_size: 50
request_timeout: 600  # 10 minutes
num_workers: 2  # Number of parallel queue workers (default: 2 for local deployment)

# Redis settings (from environment variables if set)
# REDIS_HOST: localhost (default)
# REDIS_PORT: 6379 (default)
# REDIS_PASSWORD: redis_password (default)
# REDIS_DB: 0 (default)

# Model definitions
# NOTE: Containers must be created beforehand using the docker run commands
# from CACHYOS_SETUP.md. This proxy only starts/stops existing containers.

models:
  gpt-oss-120b:
    container_name: gpt-oss-server
    backend_url: http://localhost:8080/v1
    health_url: http://localhost:8080/health
    startup_timeout: 180  # 3 minutes (120B model, ROCm 7 RC - 2x faster PP)

  qwen3-coder-30b:
    container_name: qwen-server
    backend_url: http://localhost:8080/v1
    health_url: http://localhost:8080/health
    startup_timeout: 120  # ~2 minutes for 30B model with 262K context

  dolphin-mistral-24b:
    container_name: dolphin-server
    backend_url: http://localhost:8080/v1
    health_url: http://localhost:8080/health
    startup_timeout: 90  # ~1.5 minutes for 24B model

  dolphin-mistral-24b-fast:
    container_name: dolphin-fast-server
    backend_url: http://localhost:8080/v1
    health_url: http://localhost:8080/health
    startup_timeout: 60  # ~1 minute for Q4 24B model

  lfm2-8b:
    container_name: lfm2-server
    backend_url: http://localhost:8080/v1
    health_url: http://localhost:8080/health
    startup_timeout: 60  # 1 minute (actual ~4-6 seconds, conservative timeout)

  gpt-oss-20b:
    container_name: gpt-oss-20b-server
    backend_url: http://localhost:8080/v1
    health_url: http://localhost:8080/health
    startup_timeout: 60  # 1 minute (ROCm 7 RC - 2x faster PP)

  qwen3-30b-thinking:
    container_name: qwen-thinking-server
    backend_url: http://localhost:8080/v1
    health_url: http://localhost:8080/health
    startup_timeout: 60  # 1 minute (actual ~30-45 seconds)

  jamba-reasoning-3b:
    container_name: jamba-reasoning-server
    backend_url: http://localhost:8080/v1
    health_url: http://localhost:8080/health
    startup_timeout: 60  # 1 minute (actual ~5-10 seconds)

  qwen3-vl-30b:
    container_name: qwen3-vl-server
    backend_url: http://localhost:8080/v1
    health_url: http://localhost:8080/health
    startup_timeout: 180  # 3 minutes (30B vision model with transformers + ROCm)

  lfm2-vl-1.6b:
    container_name: lfm2-vl-server
    backend_url: http://localhost:8080/v1
    health_url: http://localhost:8080/health
    startup_timeout: 30  # 30 seconds (1.6B vision model, Q8 GGUF, fast load)

  qwen3-30b-instruct:
    container_name: qwen-instruct-server
    backend_url: http://localhost:8080/v1
    health_url: http://localhost:8080/health
    startup_timeout: 60  # 1 minute (ROCm 7 RC - 2x faster PP)

  gpt-oss-20b-neoplus:
    container_name: gpt-oss-20b-neoplus-server
    backend_url: http://localhost:8080/v1
    health_url: http://localhost:8080/health
    startup_timeout: 60  # 1 minute (actual ~20-30 seconds, uncensored)

  gpt-oss-20b-code-di:
    container_name: gpt-oss-20b-code-di-server
    backend_url: http://localhost:8080/v1
    health_url: http://localhost:8080/health
    startup_timeout: 60  # 1 minute (actual ~20-30 seconds, code-focused)

  qwen3-6b-almost-human:
    container_name: qwen3-6b-server
    backend_url: http://localhost:8080/v1
    health_url: http://localhost:8080/health
    startup_timeout: 60  # 1 minute (actual ~20-40 seconds)

  lfm2-1.2b-tool:
    container_name: lfm2-1.2b-tool-server
    backend_url: http://localhost:8080/v1
    health_url: http://localhost:8080/health
    startup_timeout: 30  # 30 seconds (actual ~2-5 seconds)

  lfm2-1.2b-rag:
    container_name: lfm2-1.2b-rag-server
    backend_url: http://localhost:8080/v1
    health_url: http://localhost:8080/health
    startup_timeout: 30  # 30 seconds (actual ~2-5 seconds, RAG model)

  lfm2-1.2b-extract:
    container_name: lfm2-1.2b-extract-server
    backend_url: http://localhost:8080/v1
    health_url: http://localhost:8080/health
    startup_timeout: 30  # 30 seconds (actual ~2-5 seconds, extraction model)

  llama-3.2-3b:
    container_name: llama-3.2-3b-server
    backend_url: http://localhost:8080/v1
    health_url: http://localhost:8080/health
    startup_timeout: 30  # 30 seconds (actual ~5-8 seconds, 128K context)

  mxbai-embed-large-v1:
    type: embedding  # Embedding model for /v1/embeddings endpoint
    container_name: mxbai-embed-server
    backend_url: http://localhost:8081/v1
    health_url: http://localhost:8081/health
    startup_timeout: 30  # 30 seconds (actual ~3-5 seconds, embedding model with ROCm GPU)
